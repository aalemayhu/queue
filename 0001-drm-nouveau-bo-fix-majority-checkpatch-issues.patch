From 2f36a938a7e5f7b7ef0009df00c9c1523afbfc3a Mon Sep 17 00:00:00 2001
From: Alexander Alemayhu <alexander@alemayhu.com>
Date: Sun, 1 Jan 2017 02:38:53 +0100
Subject: [PATCH] drm/nouveau/bo: fix majority checkpatch issues

With this patch checkpatch output:

total: 0 errors, 1 warnings, 1631 lines checked

where it previously was

total: 1 errors, 119 warnings, 1620 lines checked

No functional change.

Signed-off-by: Alexander Alemayhu <alexander@alemayhu.com>
---
 drivers/gpu/drm/nouveau/nouveau_bo.c | 267 ++++++++++++++++++-----------------
 1 file changed, 139 insertions(+), 128 deletions(-)

diff --git a/drivers/gpu/drm/nouveau/nouveau_bo.c b/drivers/gpu/drm/nouveau/nouveau_bo.c
index dd07ca140d12..d4a454c547c4 100644
--- a/drivers/gpu/drm/nouveau/nouveau_bo.c
+++ b/drivers/gpu/drm/nouveau/nouveau_bo.c
@@ -121,7 +121,7 @@ nv10_bo_set_tiling(struct drm_device *dev, u32 addr,
 
 	if (found)
 		nv10_bo_update_tile_region(dev, found, addr, size,
-					    pitch, flags);
+					   pitch, flags);
 	return found;
 }
 
@@ -239,7 +239,8 @@ nouveau_bo_new(struct drm_device *dev, int size, int align,
 }
 
 static void
-set_placement_list(struct ttm_place *pl, unsigned *n, uint32_t type, uint32_t flags)
+set_placement_list(struct ttm_place *pl, unsigned int *n, uint32_t type,
+		   uint32_t flags)
 {
 	*n = 0;
 
@@ -256,7 +257,7 @@ set_placement_range(struct nouveau_bo *nvbo, uint32_t type)
 {
 	struct nouveau_drm *drm = nouveau_bdev(nvbo->bo.bdev);
 	u32 vram_pages = drm->device.info.ram_size >> PAGE_SHIFT;
-	unsigned i, fpfn, lpfn;
+	unsigned int i, fpfn, lpfn;
 
 	if (drm->device.info.family == NV_DEVICE_INFO_V0_CELSIUS &&
 	    nvbo->tile_mode && (type & TTM_PL_FLAG_VRAM) &&
@@ -290,8 +291,8 @@ nouveau_bo_placement_set(struct nouveau_bo *nvbo, uint32_t type, uint32_t busy)
 {
 	struct ttm_placement *pl = &nvbo->placement;
 	uint32_t flags = (nvbo->force_coherent ? TTM_PL_FLAG_UNCACHED :
-						 TTM_PL_MASK_CACHING) |
-			 (nvbo->pin_refcnt ? TTM_PL_FLAG_NO_EVICT : 0);
+			  TTM_PL_MASK_CACHING) |
+		(nvbo->pin_refcnt ? TTM_PL_FLAG_NO_EVICT : 0);
 
 	pl->placement = nvbo->placements;
 	set_placement_list(nvbo->placements, &pl->num_placement,
@@ -321,6 +322,7 @@ nouveau_bo_pin(struct nouveau_bo *nvbo, uint32_t memtype, bool contig)
 		if (nvbo->tile_flags & NOUVEAU_GEM_TILE_NONCONTIG) {
 			if (bo->mem.mem_type == TTM_PL_VRAM) {
 				struct nvkm_mem *mem = bo->mem.mm_node;
+
 				if (!list_is_singular(&mem->regions))
 					evict = true;
 			}
@@ -332,7 +334,7 @@ nouveau_bo_pin(struct nouveau_bo *nvbo, uint32_t memtype, bool contig)
 	if (nvbo->pin_refcnt) {
 		if (!(memtype & (1 << bo->mem.mem_type)) || evict) {
 			NV_ERROR(drm, "bo %p pinned elsewhere: "
-				      "0x%08x vs 0x%08x\n", bo,
+				 "0x%08x vs 0x%08x\n", bo,
 				 1 << bo->mem.mem_type, memtype);
 			ret = -EBUSY;
 		}
@@ -496,7 +498,7 @@ nouveau_bo_validate(struct nouveau_bo *nvbo, bool interruptible,
 }
 
 void
-nouveau_bo_wr16(struct nouveau_bo *nvbo, unsigned index, u16 val)
+nouveau_bo_wr16(struct nouveau_bo *nvbo, unsigned int index, u16 val)
 {
 	bool is_iomem;
 	u16 *mem = ttm_kmap_obj_virtual(&nvbo->kmap, &is_iomem);
@@ -510,7 +512,7 @@ nouveau_bo_wr16(struct nouveau_bo *nvbo, unsigned index, u16 val)
 }
 
 u32
-nouveau_bo_rd32(struct nouveau_bo *nvbo, unsigned index)
+nouveau_bo_rd32(struct nouveau_bo *nvbo, unsigned int index)
 {
 	bool is_iomem;
 	u32 *mem = ttm_kmap_obj_virtual(&nvbo->kmap, &is_iomem);
@@ -524,7 +526,7 @@ nouveau_bo_rd32(struct nouveau_bo *nvbo, unsigned index)
 }
 
 void
-nouveau_bo_wr32(struct nouveau_bo *nvbo, unsigned index, u32 val)
+nouveau_bo_wr32(struct nouveau_bo *nvbo, unsigned int index, u32 val)
 {
 	bool is_iomem;
 	u32 *mem = ttm_kmap_obj_virtual(&nvbo->kmap, &is_iomem);
@@ -574,9 +576,9 @@ nouveau_bo_init_mem_type(struct ttm_bo_device *bdev, uint32_t type,
 		break;
 	case TTM_PL_VRAM:
 		man->flags = TTM_MEMTYPE_FLAG_FIXED |
-			     TTM_MEMTYPE_FLAG_MAPPABLE;
+			TTM_MEMTYPE_FLAG_MAPPABLE;
 		man->available_caching = TTM_PL_FLAG_UNCACHED |
-					 TTM_PL_FLAG_WC;
+			TTM_PL_FLAG_WC;
 		man->default_caching = TTM_PL_FLAG_WC;
 
 		if (drm->device.info.family >= NV_DEVICE_INFO_V0_TESLA) {
@@ -597,10 +599,10 @@ nouveau_bo_init_mem_type(struct ttm_bo_device *bdev, uint32_t type,
 		if (drm->device.info.family >= NV_DEVICE_INFO_V0_TESLA)
 			man->func = &nouveau_gart_manager;
 		else
-		if (!drm->agp.bridge)
-			man->func = &nv04_gart_manager;
-		else
-			man->func = &ttm_bo_manager_func;
+			if (!drm->agp.bridge)
+				man->func = &nv04_gart_manager;
+			else
+				man->func = &ttm_bo_manager_func;
 
 		if (drm->agp.bridge) {
 			man->flags = TTM_MEMTYPE_FLAG_MAPPABLE;
@@ -609,7 +611,7 @@ nouveau_bo_init_mem_type(struct ttm_bo_device *bdev, uint32_t type,
 			man->default_caching = TTM_PL_FLAG_WC;
 		} else {
 			man->flags = TTM_MEMTYPE_FLAG_MAPPABLE |
-				     TTM_MEMTYPE_FLAG_CMA;
+				TTM_MEMTYPE_FLAG_CMA;
 			man->available_caching = TTM_PL_MASK_CACHING;
 			man->default_caching = TTM_PL_FLAG_CACHED;
 		}
@@ -644,10 +646,11 @@ static int
 nve0_bo_move_init(struct nouveau_channel *chan, u32 handle)
 {
 	int ret = RING_SPACE(chan, 2);
+
 	if (ret == 0) {
 		BEGIN_NVC0(chan, NvSubCopy, 0x0000, 1);
-		OUT_RING  (chan, handle & 0x0000ffff);
-		FIRE_RING (chan);
+		OUT_RING(chan, handle & 0x0000ffff);
+		FIRE_RING(chan);
 	}
 	return ret;
 }
@@ -658,16 +661,17 @@ nve0_bo_move_copy(struct nouveau_channel *chan, struct ttm_buffer_object *bo,
 {
 	struct nvkm_mem *node = old_mem->mm_node;
 	int ret = RING_SPACE(chan, 10);
+
 	if (ret == 0) {
 		BEGIN_NVC0(chan, NvSubCopy, 0x0400, 8);
-		OUT_RING  (chan, upper_32_bits(node->vma[0].offset));
-		OUT_RING  (chan, lower_32_bits(node->vma[0].offset));
-		OUT_RING  (chan, upper_32_bits(node->vma[1].offset));
-		OUT_RING  (chan, lower_32_bits(node->vma[1].offset));
-		OUT_RING  (chan, PAGE_SIZE);
-		OUT_RING  (chan, PAGE_SIZE);
-		OUT_RING  (chan, PAGE_SIZE);
-		OUT_RING  (chan, new_mem->num_pages);
+		OUT_RING(chan, upper_32_bits(node->vma[0].offset));
+		OUT_RING(chan, lower_32_bits(node->vma[0].offset));
+		OUT_RING(chan, upper_32_bits(node->vma[1].offset));
+		OUT_RING(chan, lower_32_bits(node->vma[1].offset));
+		OUT_RING(chan, PAGE_SIZE);
+		OUT_RING(chan, PAGE_SIZE);
+		OUT_RING(chan, PAGE_SIZE);
+		OUT_RING(chan, new_mem->num_pages);
 		BEGIN_IMC0(chan, NvSubCopy, 0x0300, 0x0386);
 	}
 	return ret;
@@ -677,9 +681,10 @@ static int
 nvc0_bo_move_init(struct nouveau_channel *chan, u32 handle)
 {
 	int ret = RING_SPACE(chan, 2);
+
 	if (ret == 0) {
 		BEGIN_NVC0(chan, NvSubCopy, 0x0000, 1);
-		OUT_RING  (chan, handle);
+		OUT_RING(chan, handle);
 	}
 	return ret;
 }
@@ -703,16 +708,16 @@ nvc0_bo_move_copy(struct nouveau_channel *chan, struct ttm_buffer_object *bo,
 			return ret;
 
 		BEGIN_NVC0(chan, NvSubCopy, 0x030c, 8);
-		OUT_RING  (chan, upper_32_bits(src_offset));
-		OUT_RING  (chan, lower_32_bits(src_offset));
-		OUT_RING  (chan, upper_32_bits(dst_offset));
-		OUT_RING  (chan, lower_32_bits(dst_offset));
-		OUT_RING  (chan, PAGE_SIZE);
-		OUT_RING  (chan, PAGE_SIZE);
-		OUT_RING  (chan, PAGE_SIZE);
-		OUT_RING  (chan, line_count);
+		OUT_RING(chan, upper_32_bits(src_offset));
+		OUT_RING(chan, lower_32_bits(src_offset));
+		OUT_RING(chan, upper_32_bits(dst_offset));
+		OUT_RING(chan, lower_32_bits(dst_offset));
+		OUT_RING(chan, PAGE_SIZE);
+		OUT_RING(chan, PAGE_SIZE);
+		OUT_RING(chan, PAGE_SIZE);
+		OUT_RING(chan, line_count);
 		BEGIN_NVC0(chan, NvSubCopy, 0x0300, 1);
-		OUT_RING  (chan, 0x00000110);
+		OUT_RING(chan, 0x00000110);
 
 		page_count -= line_count;
 		src_offset += (PAGE_SIZE * line_count);
@@ -741,17 +746,17 @@ nvc0_bo_move_m2mf(struct nouveau_channel *chan, struct ttm_buffer_object *bo,
 			return ret;
 
 		BEGIN_NVC0(chan, NvSubCopy, 0x0238, 2);
-		OUT_RING  (chan, upper_32_bits(dst_offset));
-		OUT_RING  (chan, lower_32_bits(dst_offset));
+		OUT_RING(chan, upper_32_bits(dst_offset));
+		OUT_RING(chan, lower_32_bits(dst_offset));
 		BEGIN_NVC0(chan, NvSubCopy, 0x030c, 6);
-		OUT_RING  (chan, upper_32_bits(src_offset));
-		OUT_RING  (chan, lower_32_bits(src_offset));
-		OUT_RING  (chan, PAGE_SIZE); /* src_pitch */
-		OUT_RING  (chan, PAGE_SIZE); /* dst_pitch */
-		OUT_RING  (chan, PAGE_SIZE); /* line_length */
-		OUT_RING  (chan, line_count);
+		OUT_RING(chan, upper_32_bits(src_offset));
+		OUT_RING(chan, lower_32_bits(src_offset));
+		OUT_RING(chan, PAGE_SIZE); /* src_pitch */
+		OUT_RING(chan, PAGE_SIZE); /* dst_pitch */
+		OUT_RING(chan, PAGE_SIZE); /* line_length */
+		OUT_RING(chan, line_count);
 		BEGIN_NVC0(chan, NvSubCopy, 0x0300, 1);
-		OUT_RING  (chan, 0x00100110);
+		OUT_RING(chan, 0x00100110);
 
 		page_count -= line_count;
 		src_offset += (PAGE_SIZE * line_count);
@@ -780,16 +785,16 @@ nva3_bo_move_copy(struct nouveau_channel *chan, struct ttm_buffer_object *bo,
 			return ret;
 
 		BEGIN_NV04(chan, NvSubCopy, 0x030c, 8);
-		OUT_RING  (chan, upper_32_bits(src_offset));
-		OUT_RING  (chan, lower_32_bits(src_offset));
-		OUT_RING  (chan, upper_32_bits(dst_offset));
-		OUT_RING  (chan, lower_32_bits(dst_offset));
-		OUT_RING  (chan, PAGE_SIZE);
-		OUT_RING  (chan, PAGE_SIZE);
-		OUT_RING  (chan, PAGE_SIZE);
-		OUT_RING  (chan, line_count);
+		OUT_RING(chan, upper_32_bits(src_offset));
+		OUT_RING(chan, lower_32_bits(src_offset));
+		OUT_RING(chan, upper_32_bits(dst_offset));
+		OUT_RING(chan, lower_32_bits(dst_offset));
+		OUT_RING(chan, PAGE_SIZE);
+		OUT_RING(chan, PAGE_SIZE);
+		OUT_RING(chan, PAGE_SIZE);
+		OUT_RING(chan, line_count);
 		BEGIN_NV04(chan, NvSubCopy, 0x0300, 1);
-		OUT_RING  (chan, 0x00000110);
+		OUT_RING(chan, 0x00000110);
 
 		page_count -= line_count;
 		src_offset += (PAGE_SIZE * line_count);
@@ -805,14 +810,15 @@ nv98_bo_move_exec(struct nouveau_channel *chan, struct ttm_buffer_object *bo,
 {
 	struct nvkm_mem *node = old_mem->mm_node;
 	int ret = RING_SPACE(chan, 7);
+
 	if (ret == 0) {
 		BEGIN_NV04(chan, NvSubCopy, 0x0320, 6);
-		OUT_RING  (chan, upper_32_bits(node->vma[0].offset));
-		OUT_RING  (chan, lower_32_bits(node->vma[0].offset));
-		OUT_RING  (chan, upper_32_bits(node->vma[1].offset));
-		OUT_RING  (chan, lower_32_bits(node->vma[1].offset));
-		OUT_RING  (chan, 0x00000000 /* COPY */);
-		OUT_RING  (chan, new_mem->num_pages << PAGE_SHIFT);
+		OUT_RING(chan, upper_32_bits(node->vma[0].offset));
+		OUT_RING(chan, lower_32_bits(node->vma[0].offset));
+		OUT_RING(chan, upper_32_bits(node->vma[1].offset));
+		OUT_RING(chan, lower_32_bits(node->vma[1].offset));
+		OUT_RING(chan, 0x00000000 /* COPY */);
+		OUT_RING(chan, new_mem->num_pages << PAGE_SHIFT);
 	}
 	return ret;
 }
@@ -823,14 +829,15 @@ nv84_bo_move_exec(struct nouveau_channel *chan, struct ttm_buffer_object *bo,
 {
 	struct nvkm_mem *node = old_mem->mm_node;
 	int ret = RING_SPACE(chan, 7);
+
 	if (ret == 0) {
 		BEGIN_NV04(chan, NvSubCopy, 0x0304, 6);
-		OUT_RING  (chan, new_mem->num_pages << PAGE_SHIFT);
-		OUT_RING  (chan, upper_32_bits(node->vma[0].offset));
-		OUT_RING  (chan, lower_32_bits(node->vma[0].offset));
-		OUT_RING  (chan, upper_32_bits(node->vma[1].offset));
-		OUT_RING  (chan, lower_32_bits(node->vma[1].offset));
-		OUT_RING  (chan, 0x00000000 /* MODE_COPY, QUERY_NONE */);
+		OUT_RING(chan, new_mem->num_pages << PAGE_SHIFT);
+		OUT_RING(chan, upper_32_bits(node->vma[0].offset));
+		OUT_RING(chan, lower_32_bits(node->vma[0].offset));
+		OUT_RING(chan, upper_32_bits(node->vma[1].offset));
+		OUT_RING(chan, lower_32_bits(node->vma[1].offset));
+		OUT_RING(chan, 0x00000000 /* MODE_COPY, QUERY_NONE */);
 	}
 	return ret;
 }
@@ -839,13 +846,14 @@ static int
 nv50_bo_move_init(struct nouveau_channel *chan, u32 handle)
 {
 	int ret = RING_SPACE(chan, 6);
+
 	if (ret == 0) {
 		BEGIN_NV04(chan, NvSubCopy, 0x0000, 1);
-		OUT_RING  (chan, handle);
+		OUT_RING(chan, handle);
 		BEGIN_NV04(chan, NvSubCopy, 0x0180, 3);
-		OUT_RING  (chan, chan->drm->ntfy.handle);
-		OUT_RING  (chan, chan->vram.handle);
-		OUT_RING  (chan, chan->vram.handle);
+		OUT_RING(chan, chan->drm->ntfy.handle);
+		OUT_RING(chan, chan->vram.handle);
+		OUT_RING(chan, chan->vram.handle);
 	}
 
 	return ret;
@@ -876,45 +884,45 @@ nv50_bo_move_m2mf(struct nouveau_channel *chan, struct ttm_buffer_object *bo,
 
 		if (src_tiled) {
 			BEGIN_NV04(chan, NvSubCopy, 0x0200, 7);
-			OUT_RING  (chan, 0);
-			OUT_RING  (chan, 0);
-			OUT_RING  (chan, stride);
-			OUT_RING  (chan, height);
-			OUT_RING  (chan, 1);
-			OUT_RING  (chan, 0);
-			OUT_RING  (chan, 0);
+			OUT_RING(chan, 0);
+			OUT_RING(chan, 0);
+			OUT_RING(chan, stride);
+			OUT_RING(chan, height);
+			OUT_RING(chan, 1);
+			OUT_RING(chan, 0);
+			OUT_RING(chan, 0);
 		} else {
 			BEGIN_NV04(chan, NvSubCopy, 0x0200, 1);
-			OUT_RING  (chan, 1);
+			OUT_RING(chan, 1);
 		}
 		if (dst_tiled) {
 			BEGIN_NV04(chan, NvSubCopy, 0x021c, 7);
-			OUT_RING  (chan, 0);
-			OUT_RING  (chan, 0);
-			OUT_RING  (chan, stride);
-			OUT_RING  (chan, height);
-			OUT_RING  (chan, 1);
-			OUT_RING  (chan, 0);
-			OUT_RING  (chan, 0);
+			OUT_RING(chan, 0);
+			OUT_RING(chan, 0);
+			OUT_RING(chan, stride);
+			OUT_RING(chan, height);
+			OUT_RING(chan, 1);
+			OUT_RING(chan, 0);
+			OUT_RING(chan, 0);
 		} else {
 			BEGIN_NV04(chan, NvSubCopy, 0x021c, 1);
-			OUT_RING  (chan, 1);
+			OUT_RING(chan, 1);
 		}
 
 		BEGIN_NV04(chan, NvSubCopy, 0x0238, 2);
-		OUT_RING  (chan, upper_32_bits(src_offset));
-		OUT_RING  (chan, upper_32_bits(dst_offset));
+		OUT_RING(chan, upper_32_bits(src_offset));
+		OUT_RING(chan, upper_32_bits(dst_offset));
 		BEGIN_NV04(chan, NvSubCopy, 0x030c, 8);
-		OUT_RING  (chan, lower_32_bits(src_offset));
-		OUT_RING  (chan, lower_32_bits(dst_offset));
-		OUT_RING  (chan, stride);
-		OUT_RING  (chan, stride);
-		OUT_RING  (chan, stride);
-		OUT_RING  (chan, height);
-		OUT_RING  (chan, 0x00000101);
-		OUT_RING  (chan, 0x00000000);
+		OUT_RING(chan, lower_32_bits(src_offset));
+		OUT_RING(chan, lower_32_bits(dst_offset));
+		OUT_RING(chan, stride);
+		OUT_RING(chan, stride);
+		OUT_RING(chan, stride);
+		OUT_RING(chan, height);
+		OUT_RING(chan, 0x00000101);
+		OUT_RING(chan, 0x00000000);
 		BEGIN_NV04(chan, NvSubCopy, NV_MEMORY_TO_MEMORY_FORMAT_NOP, 1);
-		OUT_RING  (chan, 0);
+		OUT_RING(chan, 0);
 
 		length -= amount;
 		src_offset += amount;
@@ -928,11 +936,12 @@ static int
 nv04_bo_move_init(struct nouveau_channel *chan, u32 handle)
 {
 	int ret = RING_SPACE(chan, 4);
+
 	if (ret == 0) {
 		BEGIN_NV04(chan, NvSubCopy, 0x0000, 1);
-		OUT_RING  (chan, handle);
+		OUT_RING(chan, handle);
 		BEGIN_NV04(chan, NvSubCopy, 0x0180, 1);
-		OUT_RING  (chan, chan->drm->ntfy.handle);
+		OUT_RING(chan, chan->drm->ntfy.handle);
 	}
 
 	return ret;
@@ -961,8 +970,8 @@ nv04_bo_move_m2mf(struct nouveau_channel *chan, struct ttm_buffer_object *bo,
 		return ret;
 
 	BEGIN_NV04(chan, NvSubCopy, NV_MEMORY_TO_MEMORY_FORMAT_DMA_SOURCE, 2);
-	OUT_RING  (chan, nouveau_bo_mem_ctxdma(bo, chan, old_mem));
-	OUT_RING  (chan, nouveau_bo_mem_ctxdma(bo, chan, new_mem));
+	OUT_RING(chan, nouveau_bo_mem_ctxdma(bo, chan, old_mem));
+	OUT_RING(chan, nouveau_bo_mem_ctxdma(bo, chan, new_mem));
 
 	page_count = new_mem->num_pages;
 	while (page_count) {
@@ -973,17 +982,17 @@ nv04_bo_move_m2mf(struct nouveau_channel *chan, struct ttm_buffer_object *bo,
 			return ret;
 
 		BEGIN_NV04(chan, NvSubCopy,
-				 NV_MEMORY_TO_MEMORY_FORMAT_OFFSET_IN, 8);
-		OUT_RING  (chan, src_offset);
-		OUT_RING  (chan, dst_offset);
-		OUT_RING  (chan, PAGE_SIZE); /* src_pitch */
-		OUT_RING  (chan, PAGE_SIZE); /* dst_pitch */
-		OUT_RING  (chan, PAGE_SIZE); /* line_length */
-		OUT_RING  (chan, line_count);
-		OUT_RING  (chan, 0x00000101);
-		OUT_RING  (chan, 0x00000000);
+			   NV_MEMORY_TO_MEMORY_FORMAT_OFFSET_IN, 8);
+		OUT_RING(chan, src_offset);
+		OUT_RING(chan, dst_offset);
+		OUT_RING(chan, PAGE_SIZE); /* src_pitch */
+		OUT_RING(chan, PAGE_SIZE); /* dst_pitch */
+		OUT_RING(chan, PAGE_SIZE); /* line_length */
+		OUT_RING(chan, line_count);
+		OUT_RING(chan, 0x00000101);
+		OUT_RING(chan, 0x00000000);
 		BEGIN_NV04(chan, NvSubCopy, NV_MEMORY_TO_MEMORY_FORMAT_NOP, 1);
-		OUT_RING  (chan, 0);
+		OUT_RING(chan, 0);
 
 		page_count -= line_count;
 		src_offset += (PAGE_SIZE * line_count);
@@ -1205,8 +1214,8 @@ nouveau_bo_move_ntfy(struct ttm_buffer_object *bo, struct ttm_mem_reg *new_mem)
 
 	list_for_each_entry(vma, &nvbo->vma_list, head) {
 		if (new_mem && new_mem->mem_type != TTM_PL_SYSTEM &&
-			      (new_mem->mem_type == TTM_PL_VRAM ||
-			       nvbo->page_shift != vma->vm->mmu->lpg_shift)) {
+		    (new_mem->mem_type == TTM_PL_VRAM ||
+		     nvbo->page_shift != vma->vm->mmu->lpg_shift)) {
 			nvkm_vm_map(vma, new_mem->mm_node);
 		} else {
 			WARN_ON(ttm_bo_wait(bo, false, false));
@@ -1230,8 +1239,8 @@ nouveau_bo_vm_bind(struct ttm_buffer_object *bo, struct ttm_mem_reg *new_mem,
 
 	if (drm->device.info.family >= NV_DEVICE_INFO_V0_CELSIUS) {
 		*new_tile = nv10_bo_set_tiling(dev, offset, new_mem->size,
-						nvbo->tile_mode,
-						nvbo->tile_flags);
+					       nvbo->tile_mode,
+					       nvbo->tile_flags);
 	}
 
 	return 0;
@@ -1349,7 +1358,8 @@ nouveau_ttm_io_mem_reserve(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem)
 			mem->bus.is_iomem = !drm->agp.cma;
 		}
 #endif
-		if (drm->device.info.family < NV_DEVICE_INFO_V0_TESLA || !node->memtype)
+		if (drm->device.info.family < NV_DEVICE_INFO_V0_TESLA
+		    || !node->memtype)
 			/* untiled */
 			break;
 		/* fallthrough, tiled memory */
@@ -1360,6 +1370,7 @@ nouveau_ttm_io_mem_reserve(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem)
 		if (drm->device.info.family >= NV_DEVICE_INFO_V0_TESLA) {
 			struct nvkm_bar *bar = nvxx_bar(&drm->device);
 			int page_shift = 12;
+
 			if (drm->device.info.family >= NV_DEVICE_INFO_V0_FERMI)
 				page_shift = node->page_shift;
 
@@ -1444,7 +1455,7 @@ nouveau_ttm_tt_populate(struct ttm_tt *ttm)
 	struct nvkm_device *device;
 	struct drm_device *dev;
 	struct device *pdev;
-	unsigned i;
+	unsigned int i;
 	int r;
 	bool slave = !!(ttm->page_flags & TTM_PAGE_FLAG_SG);
 
@@ -1454,7 +1465,8 @@ nouveau_ttm_tt_populate(struct ttm_tt *ttm)
 	if (slave && ttm->sg) {
 		/* make userspace faulting work */
 		drm_prime_sg_to_page_addr_arrays(ttm->sg, ttm->pages,
-						 ttm_dma->dma_address, ttm->num_pages);
+						 ttm_dma->dma_address,
+						 ttm->num_pages);
 		ttm->state = tt_unbound;
 		return 0;
 	}
@@ -1465,21 +1477,18 @@ nouveau_ttm_tt_populate(struct ttm_tt *ttm)
 	pdev = device->dev;
 
 #if IS_ENABLED(CONFIG_AGP)
-	if (drm->agp.bridge) {
+	if (drm->agp.bridge)
 		return ttm_agp_tt_populate(ttm);
-	}
 #endif
 
 #if IS_ENABLED(CONFIG_SWIOTLB) && IS_ENABLED(CONFIG_X86)
-	if (swiotlb_nr_tbl()) {
+	if (swiotlb_nr_tbl())
 		return ttm_dma_populate((void *)ttm, dev->dev);
-	}
 #endif
 
 	r = ttm_pool_populate(ttm);
-	if (r) {
+	if (r)
 		return r;
-	}
 
 	for (i = 0; i < ttm->num_pages; i++) {
 		dma_addr_t addr;
@@ -1510,7 +1519,7 @@ nouveau_ttm_tt_unpopulate(struct ttm_tt *ttm)
 	struct nvkm_device *device;
 	struct drm_device *dev;
 	struct device *pdev;
-	unsigned i;
+	unsigned int i;
 	bool slave = !!(ttm->page_flags & TTM_PAGE_FLAG_SG);
 
 	if (slave)
@@ -1546,7 +1555,8 @@ nouveau_ttm_tt_unpopulate(struct ttm_tt *ttm)
 }
 
 void
-nouveau_bo_fence(struct nouveau_bo *nvbo, struct nouveau_fence *fence, bool exclusive)
+nouveau_bo_fence(struct nouveau_bo *nvbo, struct nouveau_fence *fence,
+		 bool exclusive)
 {
 	struct reservation_object *resv = nvbo->bo.resv;
 
@@ -1578,6 +1588,7 @@ struct nvkm_vma *
 nouveau_bo_vma_find(struct nouveau_bo *nvbo, struct nvkm_vm *vm)
 {
 	struct nvkm_vma *vma;
+
 	list_for_each_entry(vma, &nvbo->vma_list, head) {
 		if (vma->vm == vm)
 			return vma;
@@ -1594,11 +1605,11 @@ nouveau_bo_vma_add(struct nouveau_bo *nvbo, struct nvkm_vm *vm,
 	int ret;
 
 	ret = nvkm_vm_get(vm, size, nvbo->page_shift,
-			     NV_MEM_ACCESS_RW, vma);
+			  NV_MEM_ACCESS_RW, vma);
 	if (ret)
 		return ret;
 
-	if ( nvbo->bo.mem.mem_type != TTM_PL_SYSTEM &&
+	if (nvbo->bo.mem.mem_type != TTM_PL_SYSTEM &&
 	    (nvbo->bo.mem.mem_type == TTM_PL_VRAM ||
 	     nvbo->page_shift != vma->vm->mmu->lpg_shift))
 		nvkm_vm_map(vma, nvbo->bo.mem.mm_node);
-- 
2.11.0

